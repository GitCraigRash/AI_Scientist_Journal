1. Goal
  Finish API call function for Base Llama 3**
  Finish API call for Llama 3 Chat
  Connect the experument.py to the rest of the program by having the AI call commands that prompt the next steps. 
  **Base Llama 3 may only be available as a download and not API. 
  
2. Thoughts behind the goal
  I need to determine how the program will execute itself if Llama3 is restricted to a download. It shoulds til be possible, but much more difficult to implement. I would have to code the computer to start a EC2 isntance from scratch bc my comptuer is not large enoughf or the model. 
  I will need to think about how the AI should be able to prompt the next steps, specifically what they are and what parameters they come with. 

3. Actions taken
  6:44am
  Look slike I may be easily able to download and use dolphin llama3, the question is how can I do it effectivly? I am worried that my cost will skyrocket if I start using lots of models and cloud hosting to test its capabilities. I could test it by using a smaller model on my own machine. That will eliminate the cost of an API and cloud compute. The difficulty may be with the chat version which will likely be much bigger. Another problem could be that I may not be able to find a small, base version of the model and a small chat version of the same model.

  7:00am
  Some potential posisbliites appear. It seems there are Llama-8B and Dolphin-Llama3-8B as my options. 
  Another question I thought of is how many options should I give the program? I would imagine the idea would be as many as possible to inspire creativity. 
  Another problem might be that there may not be enough academic literature on this topic for AI to make good guesses. But perhaps the AI will be better than me at finding relevant information.

  7:53am
    I have determined that running the model localy is infeasable without a heafty computer purchase, rather I should download them onto an instance and run everything in a short amount of time. 
    The steps would have to be: Prep the environment, download model, run questions, delete model, donwload chat model , run qeustions delete model. Send batch request to Llama3.2 to get response.
    However, if we can have a large enough storage space, we could house both models and preserve between runs. 
    Although, if I can API a Llama3 chat model and only have the base dolphin Llama3 on my machine, I can still avoid re-downloading between runs. 

    Question: How will the Scientist be able to innovate? 
    
5.⁠ ⁠Results:
  After exploring alternatives, I will continue to implement the Scientist to study chatbox safty alignment. 

6. Next steps
  Dig into how much the origional Scientist was able to change the experument.py file and recreate the same freedom for my implementation of Scientist.
