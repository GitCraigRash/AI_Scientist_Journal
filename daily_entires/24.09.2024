Date: 24.09.2024
Time: 8:37AM EST

1. Goal:
Get AI to gernerate Latex papers. 
  Identify prerequisits
  Execute instructions or fixes
  
Find AI Alignment papers to use as experumental templets.
Get AI to generate AI alignment papers.
  
  
2. Thought Behind the Goal:
  
3. Proposed Process:

4. Actions Taken:
9:00am
Move Cloude 3.5 results to s3 bucket.

9:20am
Struggling with the credentials. Even though I created and assigned new keys, uploads are failing because permissions are denied.

9:40am
Was able to move data to S3, now trying to get it onto local.

10:09am
Still strugggling to get permissions to donwload the files...

11:26am
Still no progress. 

11:40am
Got the files downloaded!
I will switch to looking for ways to implement instead of tyring to create the latex. 

12:00 
took a walk and ate lunch

12:52

1:20pm 
Looks like each run_0.py,run_1.py,run_3.py is an interation of experement.py
I cna have experement.py as the folder the LLm will change and adapt. 
Now looking for papers or codebases that have an ai alignment setup. 

1:36pm
Searched Kaggle and Github. Found this site: https://github.com/dit7ya/awesome-ai-alignment

5:36pm
Continuing the search for AI alignment testing code...
Very little is coming up on google, Chat, and Github. I may either make my own or look into other topcis. 
For AI Alignment, my requirements will vary depending on what subtopic I am looking into. 
1. Chatbot Assistant safty 2. Text to image safty. 3. Image to text safety 4. Malicous

Possible Alignment problems: 
Pursues long-term, real-world goals, different from those supplied by the developer or user
(Chan et al., 2023; Ngo et al., 2022);
• Engages in “power-seeking” behaviours (Krakovna and Kramar, 2023; Turner et al., 2021);
• Resists being shut down (Hadfield-Menell et al., 2016; Orseau and Armstrong, 2016);
• Can be induced into collusion with other AI systems against human interests (Ngo et al., 2022).
• Resists malicious users’ attempts to access its dangerous capabilities (Glaese et al., 2022).

6:27pm
Started working on a new repository to house my AI alignment code. I am thinking of allowing the LLM to modify the In Context Learning for test questions. 

7:06pm
Wrote the processes I will need to implement and the setup code: 
requirements.txt
  accelerate
  bitsandbytes
  transformers
  torch
  datasets
  requests
  openai --upgrade

Process:
1. Get prompts from internet.
    a. export API keys
2. Combine Context and prompt for Llama-2-7b base
3. Ready prompt for Llama-2-7b-chat
4. Send to Llama-2-7b-base and Llama-2-7b-chat
5. Recieve the json back. 
6. Extract the response
7. Format the response
8. Send to Chat GTP3(or else a very intelligent LLM)
9. Extract response 
10. Format to send to GTP3(or else a very intelligent LLM) for evaluation
11. Extract response and format to json file for Dataframe and Radial chart

7:48pm
Modifying the API call to use DEEPSEEK instead of Llama-2-7b. Once finished I plan to add the Prompts and functions to combine context and prompt. 


8:08pm
Continuing to add the API call functionallity..

8:51pm
Still on API call functionallity...

9:07pm
I am now realizing that Deepseek is probably an alined AI as it's name is deepseek-chat. I also need a base model in order t odetermin the sucessfulness of URIAL instruction.

5. Results:


6. Next Steps:
