Remember this instance: g4dn.xlarge


Date: 21.09.2024
Time: ~6:40am

1. Goal
Activate and run AI_Scientist on Amazon Web Service.
  
2. Thought Behind the Goal:
I received an email from AWS that my instance increase has been approved. However, when I tried to activate it I found there to be many additional options and steps I needed to complete before I could access the terminal. However, when I finally got to try the code, the computer did not recongize any of the commands. I could not get the cloud instance to run anything. This problem will need to be fixed if I am to run AWS.


3. Proposed Process:
To watch some tutuorial videos on launching an instance. 
Launch the instance and run the code. 

4. Actions Taken:
6:45am
I watched two videos about what VPC's, target groups, and load balancers. 
In the end, I discovered they were completely unnessessary to what I need. 
However, in the process I discovered that the instance I launched was the wrong type.
~7:00
I attempted to launch the correct isntance, but it failed. The vCPU limit had not been increased. 
7:20am
Asked chat how to increase the limit, and it pointed me to a URL which I used to find the quota increase form. This was different than the previous method I have been asking for increases. 
Chat said the process could take a few minutes to a few hours. 
Meanwhile, I will watch more tutorials for AWS. I think this will be more productive than scanning the source code.

8:46am
Surpsingly, I could not find any good videos about setting up an AI in Amazon Bedrock. It seems that Bedrock works by only having a certain models available. This means I need to find another model than Deepseek.

9:04am
Found some videos on how to run Amazon Bedrock, and its very confusing because A) it does not use the command line at all, 2. It 

9:56am
It seems like bedrock does not use command lines at all. Which seems to contradict the set-up instructions for AI_Scientist. How do they expect me to store the program's files in this way?

7:00pm
Started to launch a instance on AWS with g4dn.xlarge instance. Installation is going smoothly until I try to install latax. It seems to be stuck...
Installation finished. 

7:40pm
Working assigning the Deepseek_api_key. Following the rest of the isntructions to run 2d_diffusion
git clone https://github.com/gregversteeg/NPEET.git
cd NPEET
pip install .
pip install scikit-learn
# Set up 2D Diffusion baseline run
cd templates/2d_diffusion && python experiment.py --out_dir run_0 && python plot.py

(ai_scientist) [root@ip-172-31-73-238 AI-Scientist]# cd templates/2d_diffusion && python experiment.py --out_dir run_0 && python plot.py
Training model...
Training model...
Training model...
Training model...

7:51pm
Hmmmm.... Did it do anything beside training? Checking files. No new latex files.  :(
Lets see if there are more instructions... Executing next step:
python launch_scientist.py --model "deepseek-coder-v2-0724" --experiment 2d_diffusion --num-ideas 2

:D Generating Ideas!!!

hmmm, I did not give it the Semantic Scolar API.. that may cause a problem.
It is causing problems. I will stop the run, add the API, and restart.

8:03pm
Restarting.

8:18pm
I found a curiosity. While checking to see if the run had produced any papers, I went to the AI-Scientist/review_iclr_bench
/iclr_parsed/ file and found that nearly every .txt file starting with "z" did not appear in the origional folder "iclr_parsed". Double checking their origin now.
While searching, I found a bug in cmd + f while searching the AWS terminal. It only highlighted/found items within the field of view. While curious, it does not change the fact that items found in the new folder are not found in the old folder.


zyrhwrd9EYs.txt
zxEfpcmTDnF.txt
zuqcmNVK4c2.txt
zuDmDfeoB_1.txt
zrW-LVXj2k1.txt
zRJu6mU2BaE.txt
zq1iJkNk3uN.txt
ziRLU3Y2PN_.txt
zbZL1s-pBF.txt
zaALYtvbRlH.txt
zz9hXVhf40.txt

I am going on the assumption that these papers are generated. I am now trying to download them. Chat is proposing a seemingly complicated way:
scp -i /path/to/your/key.pem ec2-user@<instance-ip>:/path/to/remote/file /path/to/local/destination
Where the files are sent via the .ssh key, the path to the user's(me) instance, the path to the target file, and the path to the destination on the local machine. 
My difficulty is knowing what each of these are. Finding them now. 

.pem file:
/Users/craigrashjr/Downloads/AI_SCI_10.pem
path to target:
/root/miniconda3/AI-Scientist/review_iclr_bench/iclr_parsed/zz9hXVhf40.txt
Public IP address:
3.238.127.110
Path to destination:
/Users/craigrashjr/Downloads/zz9hXVhf40.txt

10:50pm
Spent an aggregious amount of time trying to use to terminal to download the files to my computer. I was trying to avoid compying them by hand because the termianl keeps trunkating them. No success. Copy/paste might have been faster!



5. Results:
Amazon EC2 is working!
I have yet to determine if these text files are new papers. But what is certain is that they do not appear in the origional program example papers. 

6. Next Steps:
Determine if the text files are origional. 
Find the way to transform them into latex. 
Modify code to research AI alignment.
